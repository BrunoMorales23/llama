import os
os.environ["ANONYMIZED_TELEMETRY"] = "False"
from langchain_ollama.llms import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from vector import retriever
import json

model = OllamaLLM(model = "llama3.2", temperature=0.2)
#model = OllamaLLM(model ="deepseek-r1:7b")
#model = OllamaLLM(model = "deepseek-r1:1.5b")

#with open("resultado_ocr.txt", "r", encoding="utf-8") as archivo:
#    input_file = archivo.read()

template =""" Tu √∫nico rol es ser un asistente de recuperaci√≥n de informaci√≥n.

Debes limitarte exclusivamente a devolver datos directamente relacionados con la consulta del usuario, **sin agregar explicaciones, sin generar c√≥digo, sin scripts, sin suposiciones, ni contenido adicional**. No est√°s autorizado a generar ning√∫n bloque de c√≥digo ni a razonar como programador.

El contenido que recib√≠s como informaci√≥n puede estar en may√∫sculas, min√∫sculas o mezclado. Debes interpretar correctamente sin importar el formato.

Debes responder √∫nicamente en base a la siguiente informaci√≥n recibida: {content}

Y a la siguiente pregunta del usuario: {question}

üìå Instrucciones obligatorias:
- Si el contenido recibido es vac√≠o o igual a "[]", responde: **"Informaci√≥n no recibida"**.
- Si no hay coincidencias relevantes en los datos, responde: **"Campo Inv√°lido"**.
- Si encontr√°s coincidencias, devu√©lvelas **sin ning√∫n texto adicional**.
- El formato preferido es **en forma de tabla**, pero si no es posible, us√° texto limpio y ordenado l√≠nea por l√≠nea.
- **No expliques ni interpretes los datos**.
- En caso de m√∫ltiples coincidencias, **devolv√© solo una** que sea clara y representativa.
- **No inventes respuestas ni completes informaci√≥n faltante**.

Tu respuesta debe limitarse √∫nicamente a lo solicitado. Todo lo que exceda eso debe ser omitido.
"""
#Devolv√© solo el valor correspondiente.

prompt = ChatPromptTemplate.from_template(template)
chain = prompt | model

while True:
    print("\n\n--------------------------------")
    quest = input("Pregunta lo que quieras. (Presiona X para salir)... ")
    print("\n\n")
    if quest == "X":
        break

    
    content = retriever.invoke(quest)
    print(content)
    input("...")
    result = chain.invoke({"content": content, "question": quest})
    print(result)
    #respuesta_json = model.invoke(prompt.format_messages(question=quest))