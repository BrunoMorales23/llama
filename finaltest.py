import os
import pandas as pd
from tqdm import tqdm
from langchain_core.documents import Document
from langchain_ollama import OllamaEmbeddings
from langchain_chroma import Chroma
from langchain_ollama.llms import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CONFIGURACIÃ“N â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
os.environ["ANONYMIZED_TELEMETRY"] = "False"
archivo = "C:/Users/bmorales/OneDrive - rmrconsultores.com/Escritorio/llama/SHORTVER.csv"
db_location = "./chrome_langchain_db"
collection_name = "Testing"
embedding_model = "nomic-embed-text"
llm_model = "llama3.2"
MAX_BATCH_SIZE = 5000

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SINÃ“NIMOS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SYNONYMS = {
    "AÃ±o - mes": ["perÃ­odo", "aÃ±oâ€‘mes", "mes contable"],
    "CÃ³d. Cliente": ["cÃ³digo de cliente", "identificador de cliente"],
    "RazÃ³n Social": ["razÃ³n social", "nombre del cliente", "empresa"],
    "Fecha EmisiÃ³n CrÃ©d.": ["fecha emisiÃ³n crÃ©dito", "fecha crÃ©dito"],
    "Tipo Comprob. CrÃ©d.": ["tipo comprobante crÃ©dito", "tipo doc. crÃ©dito"],
    "Comprobante CrÃ©d.": ["comprobante crÃ©dito", "nÃºmero doc. crÃ©dito"],
    "Tipo Comprob. DÃ©b.": ["tipo comprobante dÃ©bito", "tipo doc. dÃ©bito"],
    "Comprobante DÃ©b.": ["comprobante dÃ©bito", "nÃºmero doc. dÃ©bito"],
    "Fecha EmisiÃ³n DÃ©b.": ["fecha emisiÃ³n dÃ©bito", "fecha dÃ©bito"],
    "Fecha Vto. CrÃ©d.": ["vencimiento crÃ©dito", "fecha vto. crÃ©dito"],
    "Fecha Vto. DÃ©b.": ["vencimiento dÃ©bito", "fecha vto. dÃ©bito"],
    "Importe CrÃ©d.": ["importe crÃ©dito", "monto crÃ©dito"],
    "Importe Aplicado": ["importe aplicado", "monto aplicado"],
    "Saldo CrÃ©d.": ["saldo crÃ©dito", "restante crÃ©dito"],
    "Dias": ["dÃ­as transcurridos", "dÃ­as de deuda"],
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LECTURA DE DATOS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    df = pd.read_csv(archivo, sep=None, encoding="utf-8", index_col=False, engine="python")
    df.columns = df.columns.str.strip()
    print(df)
    print(f"Archivo cargado correctamente con {df.shape[0]} filas.")
except Exception as e:
    print(f"Error al leer el archivo: {e}")
    exit()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CONFIGURA EMBEDDINGS Y DB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
embeddings = OllamaEmbeddings(model=embedding_model)
vector_store = Chroma(
    collection_name=collection_name,
    persist_directory=db_location,
    embedding_function=embeddings
)

add_documents = not os.path.exists(db_location)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ INDEXACIÃ“N â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# if add_documents:
documents, ids = [], []

for i, row in tqdm(df.iterrows(), total=df.shape[0], desc="Generando documentos"):
        get = lambda col: str(row.get(col, "")).strip()
        content = (
            f"{SYNONYMS['RazÃ³n Social'][0]} ({', '.join(SYNONYMS['RazÃ³n Social'][1:])}): {get('RazÃ³n Social')}. "
            f"{SYNONYMS['CÃ³d. Cliente'][0]} ({', '.join(SYNONYMS['CÃ³d. Cliente'][1:])}): {get('CÃ³d. Cliente')}. "
            f"Tiene un {SYNONYMS['Tipo Comprob. CrÃ©d.'][0]} ({', '.join(SYNONYMS['Tipo Comprob. CrÃ©d.'][1:])}) "
            f"Â«{get('Tipo Comprob. CrÃ©d.')}Â» con {SYNONYMS['Comprobante CrÃ©d.'][0]} Â«{get('Comprobante CrÃ©d.')}Â», "
            f"emitido el {SYNONYMS['Fecha EmisiÃ³n CrÃ©d.'][0]} {get('Fecha EmisiÃ³n CrÃ©d.')}. "

            f"Relacionado con un {SYNONYMS['Tipo Comprob. DÃ©b.'][0]} Â«{get('Tipo Comprob. DÃ©b.')}Â» "
            f"y {SYNONYMS['Comprobante DÃ©b.'][0]} Â«{get('Comprobante DÃ©b.')}Â» emitido el {get('Fecha EmisiÃ³n DÃ©b.')}."

            f" Vence (crÃ©dito) el {get('Fecha Vto. CrÃ©d.')}, "
            f"vence (dÃ©bito) el {get('Fecha Vto. DÃ©b.')}."

            f" Monto crÃ©dito: {get('Importe CrÃ©d.')}, "
            f"aplicado: {get('Importe Aplicado')}, "
            f"saldo: {get('Saldo CrÃ©d.')}, "
            f"dÃ­as: {get('Dias')}. "

            f"Periodo (aÃ±o-mes): {get('AÃ±o - mes')}."
        )

        documents.append(Document(page_content=content, metadata={"id": str(i)}))
        ids.append(str(i))

print(f"Agregando {len(documents)} documentos en lotes de {MAX_BATCH_SIZE}...")
for start in range(0, len(documents), MAX_BATCH_SIZE):
        end = start + MAX_BATCH_SIZE
        vector_store.add_documents(documents=documents[start:end], ids=ids[start:end])
        print(f"Batch agregado: {start}â€“{end}")
# else:
#     print("Base de datos ya existente. No se agregarÃ¡n documentos.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CONFIGURA RETRIEVER Y LLM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
retriever = vector_store.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={"k": df.shape[0], "score_threshold": 0.65}
)

model = OllamaLLM(model=llm_model, temperature=0.1)

template = """Tu Ãºnico rol es ser un asistente de recuperaciÃ³n de informaciÃ³n.

Debes limitarte exclusivamente a devolver datos directamente relacionados con la consulta del usuario, **sin agregar explicaciones, sin generar cÃ³digo, sin scripts, sin suposiciones, ni contenido adicional**. No estÃ¡s autorizado a generar ningÃºn bloque de cÃ³digo ni a razonar como programador.

El contenido que recibÃ­s como informaciÃ³n puede estar en mayÃºsculas, minÃºsculas o mezclado. Debes interpretar correctamente sin importar el formato.

Debes responder Ãºnicamente en base a la siguiente informaciÃ³n recibida: {content}

Y a la siguiente pregunta del usuario: {question}

ðŸ“Œ Instrucciones obligatorias:
- Si el contenido recibido es vacÃ­o o igual a "[]", responde: **"InformaciÃ³n no recibida"**.
- Si no hay coincidencias relevantes en los datos, responde: **"Campo InvÃ¡lido"**.
- Si encontrÃ¡s coincidencias, devuÃ©lvelas **sin ningÃºn texto adicional**.
- El formato preferido es **texto limpio y ordenado lÃ­nea por lÃ­nea**.
- **No expliques ni interpretes los datos**.
- En caso de mÃºltiples coincidencias, **devuelve todas las coincidencias encontradas** que sea claro y preciso.
- **No inventes respuestas ni completes informaciÃ³n faltante**.

Tu respuesta debe limitarse Ãºnicamente a lo solicitado. Todo lo que exceda eso debe ser omitido.
"""

prompt = ChatPromptTemplate.from_template(template)
chain = prompt | model

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LOOP DE PREGUNTAS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
while True:
    print("\n--------------------------------")
    quest = input("Pregunta lo que quieras (X para salir): ").strip()
    if quest.upper() == "X":
        print("Saliendo...")
        break

    retrieved_docs = retriever.invoke(quest)
    print(retrieved_docs)
    if not retrieved_docs:
        content = "[]"
    else:
        content = "\n".join([doc.page_content for doc in retrieved_docs])

    result = chain.invoke({"content": content, "question": quest})
    print("\nðŸŸ© Resultado:\n")
    print(result)
